{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class MarkovDP contains the following attributes:\n",
    "1)Number of states  : s\n",
    "2)Number of actions : a\n",
    "3)Dimension of parameter space : d\n",
    "3)State Space\n",
    "4)Action Space\n",
    "5)Transition probability matrix of size (a,s,s)\n",
    "6)Reward matrix (a,s,s)\n",
    "7) Feature approximation vector of size (a,s,d) for score function and policy update\n",
    "8) Feature approximation vector phi of size (s,d) for Temporal Difference Error update\n",
    "9) policy pi of size (s,a)\n",
    "'''\n",
    "class MarkovDP:\n",
    "    \n",
    "    def __init__(self,s,a,d,T, discount_factor):\n",
    "        self.num_state             = s\n",
    "        self.num_action            = a\n",
    "        self.dimension             = d\n",
    "        self.states                = np.array(range(0,s))\n",
    "        self.actions               = np.array(range(0,a))\n",
    "        self.transitions           = np.zeros((a,s,s))\n",
    "        self.rewards               = np.zeros((a,s,s))\n",
    "        self.feature_approx        = np.zeros((a,s,d))\n",
    "        self.phi                   = np.zeros((s,d))\n",
    "        self.pi                    = np.zeros((s, a))\n",
    "        self.optimal_pi            = np.zeros((s, a))\n",
    "        self.TDerror               = np.zeros((a,s,s))\n",
    "        self.xi                    = np.zeros(s)\n",
    "        self.discount_factor       = discount_factor\n",
    "        self.thetas                = np.zeros((T+1, d))\n",
    "    \n",
    "    def test_mdp(self):\n",
    "        self.transitions[0] = [[0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1], [1,0,0,0,0]]\n",
    "        self.transitions[1] = [[1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1]]\n",
    "        self.transitions[2] = [[0,0,0,0,1], [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0]]\n",
    "        self.feature_approx = np.random.rand(self.num_action, self.num_state, self.dimension)\n",
    "        self.phi = np.random.rand(self.num_state, self.dimension)\n",
    "        self.thetas[0] = np.random.rand(self.dimension)\n",
    "        self.xi = np.full((self.num_state), (1.0/self.num_state))\n",
    "        for i in range(0,5):\n",
    "            for j in range(0,5):\n",
    "                self.rewards[0][i][j] = 0.2\n",
    "                self.rewards[1][i][j] = 0.1\n",
    "                self.rewards[2][i][j] = 0\n",
    "        for i in range(0, self.num_state):\n",
    "            self.pi[i] = np.full((self.num_action), (1.0/self.num_action))\n",
    "            self.optimal_pi[i] = np.full((self.num_action), (1.0/self.num_action))\n",
    "            \n",
    "    def initialize_mdp(self):    \n",
    "        self.rewards = np.random.rand(self.num_action, self.num_state, self.num_state)\n",
    "        self.feature_approx = np.random.rand(self.num_action, self.num_state, self.dimension)\n",
    "        self.phi = np.random.rand(self.num_state, self.dimension)\n",
    "        self.thetas[0] = np.random.rand(self.dimension)\n",
    "        self.xi = np.full((self.num_state), (1.0/self.num_state))\n",
    "        for i in range (0, self.num_action):\n",
    "            for j in range(0, self.num_state):\n",
    "                self.transitions[i][j] = np.random.dirichlet(np.ones(self.num_state, dtype = np.int8),size=1)\n",
    "        for i in range(0, self.num_state):\n",
    "            self.pi[i] = np.random.dirichlet(np.ones(self.num_action, dtype = np.int8),size=1)\n",
    "            self.optimal_pi[i] = np.random.dirichlet(np.ones(self.num_action, dtype = np.int8),size=1)\n",
    "    def score_function(self, action, state):\n",
    "        score = self.feature_approx[action][state]\n",
    "        for i in range(0, self.num_action):\n",
    "            score = score - (self.pi[state][i] * self.feature_approx[i][state])\n",
    "        return score\n",
    "    \n",
    "    def update_policy(self, actor_parameter):\n",
    "        for i in range(0, self.num_state):\n",
    "            sum = 0\n",
    "            for j in range(0, self.num_action):\n",
    "                sum += math.exp(np.dot(actor_parameter, self.feature_approx[j][i]))\n",
    "            for j in range(0, self.num_action):\n",
    "                self.pi[i][j] = ((math.exp(np.dot(actor_parameter, self.feature_approx[j][i]))) / sum)\n",
    "    def policy_eval(self, pi, gamma):\n",
    "        policy_rewards = np.zeros(self.num_state)\n",
    "        for i in range(0, self.num_state):\n",
    "            sum = 0\n",
    "            for j in range(0, self.num_action):\n",
    "                reward_sum = 0\n",
    "                for k in range(0, self.num_state):\n",
    "                    reward_sum += (self.rewards[j][i][k] * self.transitions[j][i][k])\n",
    "                sum += (pi[i][j] * reward_sum)\n",
    "            policy_rewards[i] = sum\n",
    "        policy_transitions = np.zeros((self.num_state, self.num_state))\n",
    "        for i in range(0, self.num_state):\n",
    "            for j in range(0, self.num_state):\n",
    "                sum = 0\n",
    "                for k in range(0, self.num_action):\n",
    "                    sum += (self.transitions[k][i][j] * pi[i][k])\n",
    "                policy_transitions[i][j] = sum\n",
    "        value_func = (np.dot(np.linalg.inv(np.identity(self.num_state) - (gamma * policy_transitions)),policy_rewards))\n",
    "        return value_func\n",
    "    \n",
    "    def optimal_policyValue(self):\n",
    "        policy_stable = False\n",
    "        value_func = self.policy_eval(self.optimal_pi, self.discount_factor)\n",
    "        while(not policy_stable):\n",
    "            policy_stable= True\n",
    "            for i in range(0, self.num_state):\n",
    "                old_action = np.random.choice(self.actions, p = self.optimal_pi[i])\n",
    "                max = np.zeros(2)\n",
    "                for j in range(0, self.num_action):\n",
    "                    sum = 0\n",
    "                    for k in range(0, self.num_state):\n",
    "                        sum += (self.transitions[j][i][k] * (self.rewards[j][i][k] + (self.discount_factor * value_func[k])))\n",
    "                    if (sum > max[0]):\n",
    "                        max[0] = sum\n",
    "                        max[1] = j\n",
    "                policy_dist = np.zeros(self.num_action)\n",
    "                policy_dist[int(max[1])] = 1\n",
    "                self.optimal_pi[i] = policy_dist\n",
    "                if (old_action != np.random.choice(self.actions, p = self.optimal_pi[i])):\n",
    "                    policy_stable = False\n",
    "        value_func = self.policy_eval(self.optimal_pi, self.discount_factor)\n",
    "        return value_func\n",
    "    def minibatch_TD(self, sIni, critic_stepsize, T_current, M):\n",
    "        states = np.zeros((T_current, M+1), dtype = int)\n",
    "        for i in range (0,T_current):\n",
    "            if (i == 0):\n",
    "                states[i][0] = sIni\n",
    "            else:\n",
    "                states[i][0] = states[i-1][M]\n",
    "            action = np.zeros(M, dtype = int)\n",
    "            for j in range(0, M):\n",
    "                action[j] = np.random.choice(self.actions, p = self.pi[states[i][j]])\n",
    "                states[i][j+1] = np.random.choice(self.states, p = ((self.discount_factor * self.transitions[action[j]][states[i][j]][:]) + ((1 - self.discount_factor) * self.xi)))\n",
    "                self.TDerror[action[j]][states[i][j]][states[i][j+1]] = self.rewards[action[j]][states[i][j]][states[i][j+1]] + (self.discount_factor * np.dot(self.phi[states[i][j+1]], self.thetas[i])) - (np.dot(self.phi[states[i][j]], self.thetas[i]))\n",
    "            sum = 0\n",
    "            for k in range(0, M):\n",
    "                sum += ((self.TDerror[action[k]][states[i][k]][states[i][k+1]]) * self.phi[states[i][k]])\n",
    "            self.thetas[i+1] = self.thetas[i] + (critic_stepsize * (1.0/M) * sum)\n",
    "        if T_current == 0:\n",
    "            return (self.thetas[T_current], sIni)\n",
    "        else:\n",
    "            return (self.thetas[T_current], states[T_current - 1][M])\n",
    "    \n",
    "    def actor_critic_alg(self, actor_stepsize, critic_stepsize, regularization, isNAc, T, B):\n",
    "        actor_parameter = np.zeros((T+1,self.dimension))\n",
    "        actor_parameter[0] = np.random.rand(self.dimension)\n",
    "        self.update_policy(actor_parameter[0])\n",
    "        states = np.zeros((T, B+1), dtype = int)\n",
    "        s0 = np.random.choice(self.states, p = self.xi)\n",
    "        fisher = np.zeros((T, self.dimension, self.dimension))\n",
    "        M = 10\n",
    "        \n",
    "        optimal_value = self.optimal_policyValue()\n",
    "        optimal_value = (np.sum(optimal_value) / self.num_state)\n",
    "        value_diff = np.zeros(T)\n",
    "        for i in range (0,T):\n",
    "            if(i == 0):\n",
    "                sIni = s0\n",
    "            else:\n",
    "                sIni = states[i-1][B]\n",
    "            self.thetas[i], states[i][0] = self.minibatch_TD(sIni, critic_stepsize, i, M)\n",
    "            action = np.zeros(B, dtype = int)\n",
    "            for j in range (0,B):\n",
    "                action[j] = np.random.choice(self.actions, p = self.pi[states[i][j]])\n",
    "                states[i][j+1] = np.random.choice(self.states, p = ((self.discount_factor * self.transitions[action[j]][states[i][j]][:]) + ((1 - self.discount_factor) * self.xi)))\n",
    "                self.TDerror[action[j]][states[i][j]][states[i][j+1]] = self.rewards[action[j]][states[i][j]][states[i][j+1]] + (self.discount_factor * np.dot(self.phi[states[i][j+1]], self.thetas[i])) - (np.dot(self.phi[states[i][j]], self.thetas[i]))\n",
    "                fisher[i] = fisher[i] + (1.0/B) *(self.score_function(action[j], states[i][j]) * self.score_function(action[j], states[i][j]))\n",
    "            sum = 0\n",
    "            for k in range(0, B):\n",
    "                sum += ((self.TDerror[action[k]][states[i][k]][states[i][k+1]]) * self.score_function(action[k], states[i][k]))\n",
    "            if(not isNAc):\n",
    "                actor_parameter[i+1] = actor_parameter[i] + (actor_stepsize * (1.0/B) * sum)\n",
    "                self.update_policy(actor_parameter[i+1])\n",
    "                value_func = self.policy_eval(self.pi, self.discount_factor)\n",
    "                value_diff[i] = (optimal_value - (np.sum(value_func) / self.num_state))\n",
    "            else:\n",
    "                actor_parameter[i+1] = actor_parameter[i] + (np.dot((actor_stepsize * np.linalg.inv(fisher[i] + (regularization * np.identity(self.dimension)))), ((1.0/B) * sum)))\n",
    "                self.update_policy(actor_parameter[i+1])\n",
    "                value_func = self.policy_eval(self.pi, self.discount_factor)\n",
    "                value_diff[i] = (optimal_value - (np.sum(value_func) / self.num_state))\n",
    "        return (actor_parameter[np.random.randint(0,T)], np.arange(T), value_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = 6\n",
    "env = MarkovDP(5,3,d,700, 0.9)\n",
    "env.initialize_mdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.30510859  2.46201275  4.90456018 -2.0458285  -0.34105585  0.92102868]\n"
     ]
    }
   ],
   "source": [
    "wT, x, y = env.actor_critic_alg(0.8, 0.8, 1, True, 700, 200)\n",
    "print(wT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.xlabel('T', fontsize=18)\n",
    "plt.ylabel('V*-V($\\pi_t$)', fontsize=18)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
