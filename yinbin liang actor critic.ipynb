{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class MarkovDP contains the following attributes:\n",
    "1)Number of states  : s\n",
    "2)Number of actions : a\n",
    "3)Dimension of parameter space : d\n",
    "4)State Space\n",
    "5)Action Space\n",
    "6)Transition probability matrix of size (a,s,s)\n",
    "7)Reward matrix (a,s,s)\n",
    "8) Feature approximation vector of size (a,s,d) for score function and policy update\n",
    "9) Feature approximation vector phi of size (s,d) for Temporal Difference Error update\n",
    "10) policy pi of size (s,a)\n",
    "11) optimal policy for testing\n",
    "12) Temporal Difference Error\n",
    "13) Initial probability distribution xi\n",
    "14) discount factor\n",
    "15) critic parameter thetas\n",
    "'''\n",
    "class MarkovDP:\n",
    "    \n",
    "    def __init__(self,s,a,d,T, discount_factor):\n",
    "        self.num_state             = s\n",
    "        self.num_action            = a\n",
    "        self.dimension             = d\n",
    "        self.states                = np.array(range(0,s))\n",
    "        self.actions               = np.array(range(0,a))\n",
    "        self.transitions           = np.zeros((a,s,s))\n",
    "        self.rewards               = np.zeros((a,s,s))\n",
    "        self.feature_approx        = np.zeros((a,s,d))\n",
    "        self.phi                   = np.zeros((s,d))\n",
    "        self.pi                    = np.zeros((s, a))\n",
    "        self.optimal_pi            = np.zeros((s, a))\n",
    "        self.TDerror               = np.zeros((a,s,s))\n",
    "        self.xi                    = np.zeros(s)\n",
    "        self.discount_factor       = discount_factor\n",
    "        self.thetas                = np.zeros((T+1, d))\n",
    "    \n",
    "    #mdp created to test the policy_eval and optimal_policyValue methods\n",
    "    def test_mdp(self):\n",
    "        self.transitions[0] = [[0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1], [1,0,0,0,0]]\n",
    "        self.transitions[1] = [[1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1]]\n",
    "        self.transitions[2] = [[0,0,0,0,1], [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0]]\n",
    "        self.feature_approx = np.random.rand(self.num_action, self.num_state, self.dimension)\n",
    "        self.phi = np.random.rand(self.num_state, self.dimension)\n",
    "        self.thetas[0] = np.random.rand(self.dimension)\n",
    "        self.xi = np.full((self.num_state), (1.0/self.num_state))\n",
    "        for i in range(0,5):\n",
    "            for j in range(0,5):\n",
    "                self.rewards[0][i][j] = 0.2\n",
    "                self.rewards[1][i][j] = 0.1\n",
    "                self.rewards[2][i][j] = 0\n",
    "        for i in range(0, self.num_state):\n",
    "            self.pi[i] = np.full((self.num_action), (1.0/self.num_action))\n",
    "            self.optimal_pi[i] = np.full((self.num_action), (1.0/self.num_action))\n",
    "     \n",
    "    #initializes MDP\n",
    "    def initialize_mdp(self):   \n",
    "      #for reproducibility \n",
    "        np.random.seed(0)   \n",
    "        self.rewards = np.random.rand(self.num_action, self.num_state, self.num_state)\n",
    "        self.feature_approx = np.random.rand(self.num_action, self.num_state, self.dimension)\n",
    "        self.phi = np.random.rand(self.num_state, self.dimension)\n",
    "        self.thetas[0] = np.random.rand(self.dimension)\n",
    "        self.xi = np.full((self.num_state), (1.0/self.num_state))\n",
    "        for i in range (0, self.num_action):\n",
    "            for j in range(0, self.num_state):\n",
    "                self.transitions[i][j] = np.random.dirichlet(np.ones(self.num_state, dtype = np.int8),size=1)\n",
    "        for i in range(0, self.num_state):\n",
    "            self.pi[i] = np.random.dirichlet(np.ones(self.num_action, dtype = np.int8),size=1)\n",
    "            self.optimal_pi[i] = np.random.dirichlet(np.ones(self.num_action, dtype = np.int8),size=1)\n",
    "    \n",
    "    #log gradient of policy\n",
    "    def score_function(self, action, state):\n",
    "        score = self.feature_approx[action][state]\n",
    "        for i in range(0, self.num_action):\n",
    "            score = score - (self.pi[state][i] * self.feature_approx[i][state])\n",
    "        return score\n",
    "    \n",
    "    #updates the policy using a softmax function\n",
    "    def update_policy(self, actor_parameter):\n",
    "        for i in range(0, self.num_state):\n",
    "            sum = 0\n",
    "            for j in range(0, self.num_action):\n",
    "                sum += math.exp(np.dot(actor_parameter, self.feature_approx[j][i]))\n",
    "            for j in range(0, self.num_action):\n",
    "                self.pi[i][j] = ((math.exp(np.dot(actor_parameter, self.feature_approx[j][i]))) / sum)\n",
    "     \n",
    "    #evaluates the policy and finds its value function\n",
    "    def policy_eval(self, pi, gamma):\n",
    "        policy_rewards = np.zeros(self.num_state)\n",
    "        for i in range(0, self.num_state):\n",
    "            sum = 0\n",
    "            for j in range(0, self.num_action):\n",
    "                reward_sum = 0\n",
    "                for k in range(0, self.num_state):\n",
    "                    reward_sum += self.rewards[j][i][k]\n",
    "                sum += (pi[i][j] * reward_sum)\n",
    "            policy_rewards[i] = sum\n",
    "        policy_transitions = np.zeros((self.num_state, self.num_state))\n",
    "        for i in range(0, self.num_state):\n",
    "            for j in range(0, self.num_state):\n",
    "                sum = 0\n",
    "                for k in range(0, self.num_action):\n",
    "                    sum += (self.transitions[k][i][j] * pi[i][k])\n",
    "                policy_transitions[i][j] = sum\n",
    "        value_func = (np.dot(np.linalg.inv(np.identity(self.num_state) - (gamma * policy_transitions)),policy_rewards))\n",
    "        return value_func\n",
    "    \n",
    "    #calculates the optimal policy for the given MDP\n",
    "    def optimal_policyValue(self):\n",
    "        policy_stable = False\n",
    "        value_func = self.policy_eval(self.optimal_pi, self.discount_factor)\n",
    "        while(not policy_stable):\n",
    "            policy_stable= True\n",
    "            for i in range(0, self.num_state):\n",
    "                old_action = np.random.choice(self.actions, p = self.optimal_pi[i])\n",
    "                max = np.zeros(2)\n",
    "                for j in range(0, self.num_action):\n",
    "                    sum = 0\n",
    "                    for k in range(0, self.num_state):\n",
    "                        sum += (self.transitions[j][i][k] * (self.rewards[j][i][k] + (self.discount_factor * value_func[k])))\n",
    "                    if (sum > max[0]):\n",
    "                        max[0] = sum\n",
    "                        max[1] = j\n",
    "                policy_dist = np.zeros(self.num_action)\n",
    "                policy_dist[int(max[1])] = 1\n",
    "                self.optimal_pi[i] = policy_dist\n",
    "                if (old_action != np.random.choice(self.actions, p = self.optimal_pi[i])):\n",
    "                    policy_stable = False\n",
    "        value_func = self.policy_eval(self.optimal_pi, self.discount_factor)\n",
    "        return value_func\n",
    "    \n",
    "    #updates critic parameter\n",
    "    def minibatch_TD(self, sIni, critic_stepsize, T_current, M):\n",
    "        states = np.zeros((T_current, M+1), dtype = int)\n",
    "        for i in range (0,T_current):\n",
    "            if (i == 0):\n",
    "                states[i][0] = sIni\n",
    "            else:\n",
    "                states[i][0] = states[i-1][M]\n",
    "            action = np.zeros(M, dtype = int)\n",
    "            for j in range(0, M):\n",
    "                action[j] = np.random.choice(self.actions, p = self.pi[states[i][j]])\n",
    "                states[i][j+1] = np.random.choice(self.states, p = ((self.discount_factor * self.transitions[action[j]][states[i][j]][:]) + ((1 - self.discount_factor) * self.xi)))\n",
    "                self.TDerror[action[j]][states[i][j]][states[i][j+1]] = self.rewards[action[j]][states[i][j]][states[i][j+1]] + (self.discount_factor * np.dot(self.phi[states[i][j+1]], self.thetas[i])) - (np.dot(self.phi[states[i][j]], self.thetas[i]))\n",
    "            sum = 0\n",
    "            for k in range(0, M):\n",
    "                sum += ((self.TDerror[action[k]][states[i][k]][states[i][k+1]]) * self.phi[states[i][k]])\n",
    "            self.thetas[i+1] = self.thetas[i] + (critic_stepsize * (1.0/M) * sum)\n",
    "        if T_current == 0:\n",
    "            return (self.thetas[T_current], sIni)\n",
    "        else:\n",
    "            return (self.thetas[T_current], states[T_current - 1][M])\n",
    "    \n",
    "    #runs ac algorithm\n",
    "    def actor_critic_alg(self, actor_stepsize, critic_stepsize, regularization, isNAc, T, B):\n",
    "        actor_parameter = np.zeros((T+1,self.dimension))\n",
    "        actor_parameter[0] = np.random.rand(self.dimension)\n",
    "        self.update_policy(actor_parameter[0])\n",
    "        states = np.zeros((T, B+1), dtype = int)\n",
    "        s0 = np.random.choice(self.states, p = self.xi)\n",
    "        fisher = np.zeros((T, self.dimension, self.dimension))\n",
    "        M = 10\n",
    "        \n",
    "        optimal_value = self.optimal_policyValue()\n",
    "        optimal_value = (np.sum(optimal_value) / self.num_state)\n",
    "        value_diff = np.zeros(T)\n",
    "        for i in range (0,T):\n",
    "            if(i == 0):\n",
    "                sIni = s0\n",
    "            else:\n",
    "                sIni = states[i-1][B]\n",
    "            self.thetas[i], states[i][0] = self.minibatch_TD(sIni, critic_stepsize, i, M)\n",
    "            action = np.zeros(B, dtype = int)\n",
    "            for j in range (0,B):\n",
    "                action[j] = np.random.choice(self.actions, p = self.pi[states[i][j]])\n",
    "                states[i][j+1] = np.random.choice(self.states, p = ((self.discount_factor * self.transitions[action[j]][states[i][j]][:]) + ((1 - self.discount_factor) * self.xi)))\n",
    "                self.TDerror[action[j]][states[i][j]][states[i][j+1]] = self.rewards[action[j]][states[i][j]][states[i][j+1]] + (self.discount_factor * np.dot(self.phi[states[i][j+1]], self.thetas[i])) - (np.dot(self.phi[states[i][j]], self.thetas[i]))\n",
    "                fisher[i] = fisher[i] + (1/B) *(self.score_function(action[j], states[i][j]) * self.score_function(action[j], states[i][j]))\n",
    "            sum = 0\n",
    "            for k in range(0, B):\n",
    "                sum += ((self.TDerror[action[k]][states[i][k]][states[i][k+1]]) * self.score_function(action[k], states[i][k]))\n",
    "            if(not isNAc):\n",
    "                actor_parameter[i+1] = actor_parameter[i] + (actor_stepsize * (1.0/B) * sum)\n",
    "                self.update_policy(actor_parameter[i+1])\n",
    "                value_func = self.policy_eval(self.pi, self.discount_factor)\n",
    "                value_diff[i] = (optimal_value - (np.sum(value_func) / self.num_state))\n",
    "            else:\n",
    "                actor_parameter[i+1] = actor_parameter[i] + (np.dot((actor_stepsize * np.linalg.inv(fisher[i] + (regularization * np.identity(self.dimension)))), ((1.0/B) * sum)))\n",
    "                self.update_policy(actor_parameter[i+1])\n",
    "                value_func = self.policy_eval(self.pi, self.discount_factor)\n",
    "                value_diff[i] = (optimal_value - (np.sum(value_func) / self.num_state))\n",
    "        return (actor_parameter[np.random.randint(0,T)], np.arange(T), value_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = 6\n",
    "env = MarkovDP(5,3,d,500, 0.9)\n",
    "env.initialize_mdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52167495 0.29525339 0.18307167]\n",
      " [0.6862333  0.00950532 0.30426139]\n",
      " [0.31732853 0.47058296 0.21208851]\n",
      " [0.66731462 0.23672456 0.09596082]\n",
      " [0.55603741 0.34891618 0.09504641]]\n",
      "[[0.28628364 0.02935092 0.68436543]\n",
      " [0.34037777 0.64706576 0.01255647]\n",
      " [0.01695073 0.98149911 0.00155016]\n",
      " [0.89843125 0.03801544 0.06355331]\n",
      " [0.02832138 0.02360287 0.94807575]]\n",
      "[ 0.37395412 -0.11330115  0.19311928  0.16644461  0.8774134   1.44955086]\n"
     ]
    }
   ],
   "source": [
    "wT, x, y = env.actor_critic_alg(0.8, 0.8, 0.5, True, 500, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.xlabel('T', fontsize=18)\n",
    "plt.ylabel('V*-V($\\pi_t$)', fontsize=18)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
